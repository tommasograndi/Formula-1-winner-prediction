---
title: "F1 Prediction"
output:
  pdf_document: default
  html_document: default
---

# FORMULA 1 CHAMPIONSHIP EDA AND WINNER PREDICTION

## Loading libraries

```{r message=FALSE, warning=FALSE}
library(ggplot2)
library(dplyr)
library(rpart)
library(corrplot)
library(treemap)
library(treemapify)
library(tree)
library(randomForest)
library(caret)
library(e1071)
library(rpart.plot)
library(car)
```

## Data loading

```{r}
df = read.csv("~/Library/Mobile Documents/com~apple~CloudDocs/UNICATT/Data analysis techniques and tools/F1_prediction/datasets/df.csv")
str(df)
```

```{r}
head(df)
```

<br>

Our starting dataframe `df` is composed by 25 columns of features that define for each row a Driver performance in a specific Race of a specific year. So for each row:

-   5 columns describe the unique observation: `raceId`, `driverId`, `constructorId`, `circuitId`, `resultId`. They were useful in the initial part of the data processing to merge multiple features from other datasets into this one main dataframe that include all the features that we later use for data analysis and prediction.

-   2 columns define the perfomance of the driver in the specific race, mainly `positionOrder` and the binary `winner`. These are the target variables of the regression and classification methods.

-   11 columns are numerical features used:

    -   `number`, number of each driver's car
    -   `grid`, starting position for the driver
    -   `points`, number of points earned in the race
    -   `laps`, number of laps in the race
    -   `fastestLapSpeed`
    -   `round`, number of the Race in the season
    -   `const_points`, number of points earned by the team before this race
    -   `const_wins`, number of wins of the team before the race
    -   `driver_age`, age of each driver
    -   `fastestLap_ms`, fastest lap in the qualification race
    -   `wins`, number of wins before the race for the driver

-   5 columns that describe categorical features:

    -   `status`, what happened in the race for the driver
    -   `driv_nationality`
    -   `fullname`, name of the driver
    -   `const_name`, name of the team
    -   `name`, name of the circuit in which the race takes place

-   2 columns for dates: `dob` is birth date for each driver, `date` is the date of the Race.

<br> <br>

# EDA and plots.

```{r}
ggplot(df, aes(x=driver_age)) + 
 geom_histogram(aes(y=after_stat(density)), colour="black", fill= '#076fa2')+
 geom_density(alpha=.2, fill="#FF6666") +
 labs(title = 'Histogram of driver age')

```

<br>

```{r}
ggplot(df, aes(x=driver_age)) +
  geom_boxplot(fill='#076fa2', color="black")+
  theme_classic() +
  labs(title = "Drivers' Age", x = "Drivers' Age")
```

<br>

```{r}
ggplot(subset(df, df$fastestLapSpeed > 100), aes(x=fastestLapSpeed)) + 
 geom_histogram(aes(y=after_stat(density)), colour="black", fill= '#076fa2')+
 geom_density(alpha=.2, fill="#FF6666") +
 labs(title = 'Histogram for fastest speed in a lap', x = 'Speed')
```

<br>

```{r}
nationality = data.frame(table(df$driv_nationality))
nationality$Var1 = as.character(nationality$Var1)
ggplot(nationality, aes(area = Freq, fill = Freq, label = Var1)) +
  geom_treemap() +
  geom_treemap_text(fontface = "italic", colour = "white", place = "centre", grow = TRUE) +
  labs(title = "Treemap of the drivers' nationality", fill = "Frequency")

```

<br>

```{r}
ggplot(df, aes(name)) + 
  labs(x = 'Circuits') +
  ggtitle('Circuits Frequency') +
  geom_bar(fill="#076fa2", position = position_dodge(0.7)) +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1, size = 6)) 
  
```

\\\\

Number of collisions for each circuit.

```{r}
#barplot showing number of collisions for each circuit
n_collision <- table(df[df$status == 'Collision', 'name'])
n_collision <- as.data.frame(n_collision)

ggplot(n_collision, aes(x = Var1, y = Freq)) + 
  labs(x = 'Circuits') +
  ggtitle('Number of collisions per circuit') +
  geom_bar(fill="#076fa2", position = position_dodge(0.7), stat = 'identity') +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1, size = 6)) 
```

<br>

Accident ratio for each circuit.

```{r}
n_accident = read.csv("~/Library/Mobile Documents/com~apple~CloudDocs/UNICATT/Data analysis techniques and tools/F1_prediction/datasets/n_accident.csv")
ggplot(n_accident, aes(x = as.character(Accident.Names), y = Accident.ratio)) + 
  labs(x = 'Circuits') +
  ggtitle('Number of collisions per circuit') +
  geom_bar(fill="#076fa2", position = position_dodge(0.7), stat = 'identity') +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1, size = 6)) 

```

<br>

Winning driver age overtime

```{r}
df_points = read.csv("~/Library/Mobile Documents/com~apple~CloudDocs/UNICATT/Data analysis techniques and tools/F1_prediction/datasets/df_points.csv")
```

```{r}
winner_Age = function(){
  df_year_winner = matrix(nrow = 1, ncol=5)
  years = seq(1958, 2022, 1)
  for (y in years){
    df_year = df_points[df_points$year == y, c(2,4,9,10)]
    id_winner = df_year[which.max(df_year$points), ]
    df_year_winner <- rbind(df_year_winner, c(y, id_winner$driverId, id_winner$points, id_winner$fullname, id_winner$driver_age))
    
  }
  df_year_winner = data.frame(df_year_winner)
  df_year_winner = df_year_winner[-1,]
  return(df_year_winner)
}
df_year_winner = winner_Age()
names(df_year_winner) = c('year', 'driverId', 'final_points', 'fullname', 'age')
df_year_winner$age = as.numeric(df_year_winner$age)
head(df_year_winner)
```

```{r}
#plot for winner's age over time
ggplot(df_year_winner, aes(x = year, y = age)) + 
  labs(x = 'Years', y = 'Age') +
  ggtitle('Winning driver age for every year') +
  geom_bar(stat = "identity") + 
  geom_col(fill = "#076fa2") +
  theme(axis.text.x = element_text(angle = 90, vjust=0.5, hjust = 1)) +
  geom_abline(slope = 0, intercept = mean(df_year_winner$age), color = 'red', linewidth = 1.5)  
```

<br>

How important is the pole position in each circuit to win the race?

```{r}
pole_ratio = read.csv("~/Library/Mobile Documents/com~apple~CloudDocs/UNICATT/Data analysis techniques and tools/F1_prediction/datasets/pole_ratio.csv")
```

```{r}
ggplot(pole_ratio, aes(x = X1, y = X2)) + 
  labs(x = 'Circuit', y = 'Ratio') +
  ggtitle('How important is the pole position') +
  geom_bar(stat = "identity") + 
  geom_col(fill = "#076fa2") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

<br>

Correlation

```{r}
#CORRELATION MATRIX for numerical features
corr_matrix = cor(df[c(7,8,9,10,11,16,20,21,22,23,25)])
corrplot(corr_matrix)
```

<br> <br>

# DEFINING AND TRAINING ML MODELS FOR PREDICTION.

```{r}
df$winner = as.factor(df$winner) #converting the column to a factor. 
```

<br>

### Partition the data for train and validation.

For partitioning the data, we decided to use a different approach. Instead of just randomly select rows from the original dataset (we recall that each row is a driver's performance in a specific race) we decided to create a block of rows for each race (with each block containing multiple observation, i.e. all the drivers that attained the race) and randomly select 80% blocks for the training sample and the rest for the testing sample. <br>

We decided to partition the data in this way in order to maintain the integrity of observations for each race. Otherwise, we could have situations in which in the testing sets there is just one driver for a Race, and so his performance would be assessed in the lack of opponents.

```{r}
races_list = unique(df$raceId)
races_train_idx = sample(length(races_list), length(races_list)*0.8)

races_train_list = races_list[races_train_idx]
races_test_list = races_list[-races_train_idx]

#creating train & test dataframes
df.train = df[df$raceId %in% races_train_list, ]
df.test = df[df$raceId %in% races_test_list, ]

```

<br>

Fixing `df.train` and `df.test` for categorical features by converting those columns in factors.

```{r}
#converting TRAIN and TEST categorical features in factors
df.train$status = as.factor(df.train$status)
df.train$driv_nationality = as.factor(df.train$driv_nationality)
df.train$fullname = as.factor(df.train$fullname)
df.train$const_name = as.factor(df.train$const_name)
df.train$name = as.factor(df.train$name)
str(df.train)

df.test$status = as.factor(df.test$status)
df.test$driv_nationality = as.factor(df.test$driv_nationality)
df.test$fullname = as.factor(df.test$fullname)
df.test$const_name = as.factor(df.test$const_name)
df.test$name = as.factor(df.test$name)
str(df.test)
```

Fixing FACTOR LEVELS in the testing dataframe. <br>

When partitioning the data, categories for nominal features are shuffled between df.train and df.test. This lead to a situation in which categories in df.test could not be present in df.train and so classification models trained on df.test would not consider other categories that are only present in df.test. <br>

With the following lines of code we overcome this problem by setting factor levels of all categorical columns of df.test to be exactly the same as df.train. Otherwise, we would encounter problems in the classifications method when predicting the new data from df.test using the model trained on df.train.

```{r}

levels(df.test$status) <- levels(df.train$status)
levels(df.test$driv_nationality) <- levels(df.train$driv_nationality)
levels(df.test$fullname) <- levels(df.train$fullname)
levels(df.test$const_name) <- levels(df.train$const_name)
levels(df.test$name) <- levels(df.train$name)
```

<br> <br>

### SCORING FUNCTIONS

The two scoring functions we created, simply calculate the accuracy for the prediction of the winner of each race.

For the regressions, the scoring functions sort the prediction of `positionOrder` for each race (by using `raceId`) and take the smallest value for the prediction (since in the `positionsOrder` the winner has position 1, hence the smallest position). If the same driver (a row of df.test) has the real value of `positionOrder` equal to 1, then the prediction is correct and the score is increased by 1. After scrolling through all races that are contained in df.test, the `accuracy_score` is defined as the ratio between score and the number of races.

For the classification, the scoring function works similarly but there is a difference. We are using the classifications models to predict `winner` defined in {0,1} but the models in action are going to assign 1 to multiple drivers in the same race (but we can have just one winner in a race). So instead we look for the greatest probability among the driver of a race to have the winner feature equal to 1. The driver with the greatest probability of having the `winner` feature equal to 1 would be the driver with the greatest probability of winning the race and hence he is the winner our function pick. The definition of score works in the same way as in the scoring function for regressions.

<br>

Defining scoring function for Regression.

```{r}
score_regression <- function(test, prediction){
  
  df.score.regres = data.frame(test, prediction)
  colnames(df.score.regres)[c(26)] = c('prediction') 
  
  racelist = unique(df.score.regres$raceId)
  
  score = 0
  for (i in 1:length(racelist)){
    race = df.score.regres[df.score.regres$raceId %in% racelist[i], ]
    pred_winner_idx = which.min(race$prediction)
    if (race$positionOrder[pred_winner_idx] == 1){
      score = score + 1
    }
  }
  
  score_ratio = score/length(racelist)
  return(score_ratio)
} 
```

Defining scoring function for Classification.

```{r}
score_classification <- function(test, prediction){
  
  df.score.class = data.frame(test, prediction)
  colnames(df.score.class)[c(26,27)] = c('pred_0', 'pred_1')  
  
  racelist = unique(df.score.class$raceId)
  
  score = 0
  for (i in 1:length(racelist)){
    race = df.score.class[df.score.class$raceId %in% racelist[i], ]
    pred_winner_idx = which.max(race$pred_1)
    if (race$positionOrder[pred_winner_idx] == 1){
      score = score + 1
    }
  }
  
  score_ratio = score/length(racelist)
  return(score_ratio)
}
```

<br>

### REGRESSION

#### Training and testing linear regression models.

For the regression model we decided to use as explanatory variables most of the numerical features as: grid, laps, fastestLapSpeed, round, const_points, const_wins, fastestLap_ms and wins that presumably would help predict and explain the final position of the driver. Some variables were excluced as points, since its high correlation with the final position (higher number of points means a higher position, were 1 is the highest position).

```{r}

#first, we train and test a linear model with all numerical features in the dataset.
#linear regression
linearmodel = lm(positionOrder ~ grid + laps + fastestLapSpeed + round + const_points + const_wins + fastestLap_ms + wins, data = df.train)
summary(linearmodel)
avPlots(linearmodel)

```

As we can see the linear model obtain an R-squared of 65% that confirms a good capacity of the model to represent variability in the original data. Also, the majority of variables are strongly significant. We can now try to run a prediction and recall 'score_regression' function to see the results of the prediction.

```{r}
predict_lm = predict(linearmodel, df.test)
score_regression(df.test, predict_lm) 


```

<br>

#### Regression tree

Decision trees are another technique that we have used both in regression and in classification. The model used in regression take as explanatory the same variables in the regression:

```{r}
predict_rt = predict(tree(positionOrder ~ grid + number + laps + fastestLapSpeed + round + const_points + const_wins + driver_age + fastestLap_ms + wins
                               , df.train), newdata = df.test)
score_regression(df.test, predict_rt) #47%
```

<br> <br>

### CLASSIFICATION

Using the features class `winner` that we have created, we want to predict the probability of class=1 (winner) or class=0 (not winner). Then we are going to sort the probabilities and pick the greater probability of class=1, hence the driver with the greatest probability of being a winner. <br>

For the classification methods, we decided to remove those categorical features as `status`, `fullname` and `const_name` that when converted in factors where causing problems in the application of the techniques, or drastically reduced performances due to their dimension (all of those 3 factors presented more than 100 levels that were not accepted by the functions or ruined the prediction accuracy).

#### BAYES CLASSIFIER

```{r}
df.nb <- naiveBayes(winner ~ . -number -positionOrder -resultId -points -status -fullname -const_name, data = df.train)
prediction_nb <- predict(df.nb, newdata = df.test, type = 'raw')
score_classification(df.test, prediction_nb) 
```

<br>

#### DECISION TREES

```{r}
df.dt = rpart(winner ~ . -number -positionOrder -resultId -points -status -fullname -const_name -dob -date, data = df.train, method="class")
prediction_dt = predict(df.dt, newdata = df.test, method="prob")
score_classification(df.test, prediction_dt) #58%
```

```{r message=FALSE, warning=FALSE}
rpart.plot(df.dt,faclen = 2)
```

<br>

#### RANDOM FOREST

```{r}
df.rf <- randomForest(winner ~ . -number -points -positionOrder -resultId -const_name -name -fullname -status, data = df.train, ntree = 200)
prediction.rf <- predict(df.rf, df.test, type = 'prob')
prediction.rf[is.na(prediction.rf)] <- 0
score_classification(df.test, prediction.rf) 
```

<br>

#### SVM classifier

```{r}
df.lsvm = svm(winner ~ ., data = df.train[, -c(5,6,8,9,12,13,15,17)], kernel = 'linear', fitted = FALSE, probability = TRUE)   #sigmoid kernel working best
prediction_svm <- predict(df.lsvm, newdata = df.test[,-c(5,6,8,9,12,13,15,17)], fitted = FALSE, probability = TRUE)
SVM_class = data.frame(attributes(prediction_svm)$probabilities)
score_classification(df.test, SVM_class)
```

### Visualize final prediction results

```{r}
accuracy_results = data.frame(c(score_regression(df.test, predict_lm), score_regression(df.test, predict_rt), score_classification(df.test, prediction_nb), score_classification(df.test, prediction_dt), score_classification(df.test, prediction.rf), score_classification(df.test, SVM_class)),c('Linear Regression', 'Regression tree','Bayes classifier','Decision tree','Random forest','SVM classifier'))
colnames(accuracy_results) = c('score', 'method')

ggplot(accuracy_results, aes(x = score, y = method)) + 
  labs(x = 'Score', y = 'Method') +
  geom_bar(stat = "identity") + 
  geom_col(fill = "#076fa2") 
```

# CONCLUSIONS

Our initial goal wasn't to analyze deeply the data that characterize a Formula 1 Race. Our goal was to rather try to predict the winner of a Race with the least possible number of numerical and categorical features.

<br> <br> <br> <br>
