---
title: "F1 Prediction"
output:
  html_document: default
  pdf_document: default
---

# FORMULA 1 CHAMPIONSHIP EDA AND WINNER PREDICTION

# Loading libraries

```{r}
library(ggplot2)
library(dplyr)
library(rpart)
library(corrplot)
library(treemap)
library(treemapify)
library(tree)
library(randomForest)
library(caret)
library(e1071)
library(rpart.plot)
```

# Data cleansing and pre-processing

```{r}
df = read.csv("~/Documents/GitHub/F1_prediction/datasets/df.csv")
```

```{r}
head(df)
```

<br> <br>

# EDA and plots.

```{r}
ggplot(df, aes(x=driver_age)) + 
 geom_histogram(aes(y=after_stat(density)), colour="black", fill= '#076fa2')+
 geom_density(alpha=.2, fill="#FF6666") +
 labs(title = 'Histogram of driver age')

```

```{r}
ggplot(df, aes(x=driver_age)) +
  geom_boxplot(fill='#076fa2', color="black")+
  theme_classic() +
  labs(title = "Drivers' Age", x = "Drivers' Age")
```

```{r}
ggplot(subset(df, df$fastestLapSpeed > 100), aes(x=fastestLapSpeed)) + 
 geom_histogram(aes(y=after_stat(density)), colour="black", fill= '#076fa2')+
 geom_density(alpha=.2, fill="#FF6666") +
 labs(title = 'Histogram for fastest speed in a lap', x = 'Speed')
```

```{r}
nationality = data.frame(table(df$driv_nationality))
nationality$Var1 = as.character(nationality$Var1)
ggplot(nationality, aes(area = Freq, fill = Freq, label = Var1)) +
  geom_treemap() +
  geom_treemap_text(fontface = "italic", colour = "white", place = "centre", grow = TRUE) +
  labs(title = "Treemap of the drivers' nationality", fill = "Frequency")

```

```{r}
ggplot(df, aes(name)) + 
  labs(x = 'Circuits') +
  ggtitle('Circuits Frequency') +
  geom_bar(fill="#076fa2", position = position_dodge(0.7)) +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1, size = 6)) 
  
```

Number of collisions for each circuit.

```{r}
#barplot showing number of collisions for each circuit
n_collision <- table(df[df$status == 'Collision', 'name'])
n_collision <- as.data.frame(n_collision)

ggplot(n_collision, aes(x = Var1, y = Freq)) + 
  labs(x = 'Circuits') +
  ggtitle('Number of collisions per circuit') +
  geom_bar(fill="#076fa2", position = position_dodge(0.7), stat = 'identity') +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1, size = 6)) 
```

Accident ratio for each circuit.

```{r}
n_accident = read.csv("~/Documents/GitHub/F1_prediction/datasets/n_accident.csv")
ggplot(n_accident, aes(x = as.character(Accident.Names), y = Accident.ratio)) + 
  labs(x = 'Circuits') +
  ggtitle('Number of collisions per circuit') +
  geom_bar(fill="#076fa2", position = position_dodge(0.7), stat = 'identity') +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1, size = 6))  
```

Winning driver age overtime

```{r}
df_points = read.csv("~/Documents/GitHub/F1_prediction/datasets/df_points.csv")
```

```{r}
winner_Age = function(){
  df_year_winner = matrix(nrow = 1, ncol=5)
  years = seq(1958, 2022, 1)
  for (y in years){
    df_year = df_points[df_points$year == y, c(2,4,9,10)]
    id_winner = df_year[which.max(df_year$points), ]
    df_year_winner <- rbind(df_year_winner, c(y, id_winner$driverId, id_winner$points, id_winner$fullname, id_winner$driver_age))
    
  }
  df_year_winner = data.frame(df_year_winner)
  df_year_winner = df_year_winner[-1,]
  return(df_year_winner)
}
df_year_winner = winner_Age()
names(df_year_winner) = c('year', 'driverId', 'final_points', 'fullname', 'age')
df_year_winner$age = as.numeric(df_year_winner$age)
head(df_year_winner)
```

```{r}
#plot for winner's age over time
ggplot(df_year_winner, aes(x = year, y = age)) + 
  labs(x = 'Years', y = 'Age') +
  ggtitle('Winning driver age for every year') +
  geom_bar(stat = "identity") + 
  geom_col(fill = "#076fa2") +
  theme(axis.text.x = element_text(angle = 90, vjust=0.5, hjust = 1)) +
  geom_abline(slope = 0, intercept = mean(df_year_winner$age), color = 'red', size = 1.5)  
```

How important is the pole position in each circuit to win the race?

```{r}
pole_ratio = read.csv("~/Documents/GitHub/F1_prediction/datasets/pole_ratio.csv")
```

```{r}
ggplot(pole_ratio, aes(x = X1, y = X2)) + 
  labs(x = 'Circuit', y = 'Ratio') +
  ggtitle('How important is the pole position') +
  geom_bar(stat = "identity") + 
  geom_col(fill = "#076fa2") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Correlation

```{r}
#CORRELATION MATRIX for numerical features
corr_matrix = cor(df[c(7,8,9,10,11,16,20,21,22,23,25)])
corrplot(corr_matrix)
```

# DEFINING AND TRAINING ML MODELS FOR PREDICTION.

Adding column winner for classification training and testing.

```{r}
for (i in 1:nrow(df)) {
  if (df[i, "positionOrder"] == 1){
    df[i, 'winner'] <- 1
  }else{
    df[i, 'winner'] <- 0
  }
}
df$winner = as.factor(df$winner) #converting the column to a factor. 
```

Defining TRAIN and TEST dataframes.

```{r}
races_list = unique(df$raceId)
races_train_idx = sample(length(races_list), length(races_list)*0.8)

races_train_list = races_list[races_train_idx]
races_test_list = races_list[-races_train_idx]

#creating train & test dataframes
df.train = df[df$raceId %in% races_train_list, ]
df.test = df[df$raceId %in% races_test_list, ]

```

Fixing data.train and data.test for categorical features.

```{r}
#converting TRAIN and TEST categorical features in factors
df.train$status = as.factor(df.train$status)
df.train$driv_nationality = as.factor(df.train$driv_nationality)
df.train$fullname = as.factor(df.train$fullname)
df.train$const_name = as.factor(df.train$const_name)
df.train$name = as.factor(df.train$name)
str(df.train)

df.test$status = as.factor(df.test$status)
df.test$driv_nationality = as.factor(df.test$driv_nationality)
df.test$fullname = as.factor(df.test$fullname)
df.test$const_name = as.factor(df.test$const_name)
df.test$name = as.factor(df.test$name)
str(df.test)

#fixing FACTOR LEVELS in the testing dataframe
#In this way we set factor levels of ALL categorical columns of df.test to be exactly the same as df.train.
levels(df.test$status) <- levels(df.train$status)
levels(df.test$driv_nationality) <- levels(df.train$driv_nationality)
levels(df.test$fullname) <- levels(df.train$fullname)
levels(df.test$const_name) <- levels(df.train$const_name)
levels(df.test$name) <- levels(df.train$name)
```

### SCORING FUNCTIONS

The two scoring functions we created, simply calculate the accuracy for the prediction of the winner of each race.

For the regressions, the scoring functions sort the prediction of positionOrder for each race (by using raceId) and take the smallest value for the prediction. If the same driver (a row of df.test) has the real value of positionOrder equal to 1, then the prediction is correct and the score is increased by 1. After scrolling through all races that are present in df.test, the accuracy_score is defined as the ratio between score and the number of races.

For the classification, the scoring function works similarly but there is a difference. We are using the classifications models to predict winner defined in {0,1} but the models in action are going to assign 1 to multiple drivers in the same race (but we can have just one winner in a race). So instead we look for the greatest probability among the driver of a race to have the winner feature equal to 1. The driver with the greatest probability of having the winner feature equal to 1 would be the driver with the greatest probability of winning the race and hence he is the winner our function pick. The definition of score works in the same way as in the scoring function for regressions.

Defining scoring function for Regression.

```{r}
score_regression <- function(test, prediction){
  
  df.score.regres = data.frame(test, prediction)
  colnames(df.score.regres)[c(26)] = c('prediction') #substitute with 25
  
  racelist = unique(df.score.regres$raceId)
  
  score = 0
  for (i in 1:length(racelist)){
    race = df.score.regres[df.score.regres$raceId %in% racelist[i], ]
    pred_winner_idx = which.min(race$prediction)
    if (race$positionOrder[pred_winner_idx] == 1){
      score = score + 1
    }
  }
  
  score_ratio = score/length(racelist)
  return(score_ratio)
} 
```

Defining scoring function for Classification.

```{r}
score_classification <- function(test, prediction){
  
  df.score.class = data.frame(test, prediction)
  colnames(df.score.class)[c(26,27)] = c('pred_0', 'pred_1')  
  
  racelist = unique(df.score.class$raceId)
  
  score = 0
  for (i in 1:length(racelist)){
    race = df.score.class[df.score.class$raceId %in% racelist[i], ]
    pred_winner_idx = which.max(race$pred_1)
    if (race$positionOrder[pred_winner_idx] == 1){
      score = score + 1
    }
  }
  
  score_ratio = score/length(racelist)
  return(score_ratio)
}
```

### REGRESSION

#### Training and testing linear regression models.

```{r}

#first, we train and test a linear model with all numerical features in the dataset.
#linear regression
linearmodel = lm(positionOrder ~ grid + number + laps + fastestLapSpeed + round + const_points + const_wins + fastestLap_ms + wins, data = df.train)
summary(linearmodel)
```

As we can see the linear model obtain an R-squared of 65% that confirms a good capacity of the model to represent variability in the original data. Also, the majority of variables are strongly significant. We can now try to run a prediction and recall 'score_regression' function to see the results of the prediction.

```{r}
predict_lm = predict(linearmodel, df.test)
score_regression(df.test, predict_lm) 


```

#### Regression tree

Decision trees are another technique that we have used both in regression and in classification. The model used in regression take explicatory features just the numerical feature written below:

```{r}
predict_rt = predict(tree(positionOrder ~ grid + number + laps + fastestLapSpeed + round + const_points + const_wins + driver_age + fastestLap_ms + wins
                               , df.train), newdata = df.test)
score_regression(df.test, predict_rt) #47%
```

<br> <br>

### CLASSIFICATION

We want to predict the probability of class=1 (winner) or class=0 (not winner). Then we are going to sort the probabilities and pick the greater probability of class=1, hence the driver with the greatest probability of being a winner. <br>

For the classification methods, we decided to remove those categorical features as status, fullname and const_name that when converted in factors where causing problems in the application of the techniques, or drastically reduced performances due to their dimension (all of those 3 factors presented more than 100 levels). <br>

#### BAYES CLASSIFIER

```{r}
df.nb <- naiveBayes(winner ~ . -positionOrder -resultId -points -status -fullname -const_name, data = df.train)
prediction_nb <- predict(df.nb, newdata = df.test, type = 'raw')
score_classification(df.test, prediction_nb) 
```

#### DECISION TREES

```{r}
df.dt = rpart(winner ~ . -positionOrder -resultId -points -status -fullname -const_name -dob -date, data = df.train, method="class")
prediction_dt = predict(df.dt, newdata = df.test, method="prob")
score_classification(df.test, prediction_dt) #58%
```

```{r}
rpart.plot(df.dt,faclen = 2)
```

#### RANDOM FOREST

```{r}
df.rf <- randomForest(winner ~ . -points -positionOrder -resultId -const_name -name -fullname -status, data = df.train, ntree = 200)
prediction.rf <- predict(df.rf, df.test, type = 'prob')
prediction.rf[is.na(prediction.rf)] <- 0
score_classification(df.test, prediction.rf) 
```

#### SVM classifier

```{r}
df.lsvm = svm(winner ~ ., data = df.train[, -c(5,8,9,12,13,15,17)], kernel = 'linear', fitted = FALSE, probability = TRUE)   #sigmoid kernel working best
prediction_svm <- predict(df.lsvm, newdata = df.test[,-c(5,8,9,12,13,15,17)], fitted = FALSE, probability = TRUE)
SVM_class = data.frame(attributes(prediction_svm)$probabilities)
score_classification(df.test, SVM_class)
```


